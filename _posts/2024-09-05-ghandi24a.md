---
title: Soft Learning Probabilistic Circuits
abstract: Probabilistic Circuits (PCs) are prominent tractable probabilistic models,
  allowing for a wide range of exact inferences. This paper focuses on the main algorithm
  for training PCs, LearnSPN, a gold standard due to its efficiency, performance,
  and ease of use, in particular for tabular data. We show that LearnSPN is a greedy
  likelihood maximizer under mild assumptions. While inferences in PCs may use the
  entire circuit structure for processing queries, LearnSPN applies a hard method
  for learning them, propagating at each sum node a data point through one and only
  one of the children/edges as in a hard clustering process. We propose a new learning
  procedure named SoftLearn, that induces a PC using a soft clustering process. We
  investigate the effect of this learning-inference compatibility in PCs. Our experiments
  show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods
  and arguably better samples. We also analyze comparable tractable models to highlight
  the differences between soft/hard learning and model querying.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ghandi24a
month: 0
tex_title: Soft Learning Probabilistic Circuits
firstpage: 273
lastpage: 294
page: 273-294
order: 273
cycles: false
bibtex_author: Ghandi, Soroush and Quost, Benjamin and de Campos, Cassio
author:
- given: Soroush
  family: Ghandi
- given: Benjamin
  family: Quost
- given: Cassio
  family: Campos
  prefix: de
date: 2024-09-05
address:
container-title: Proceedings of The 12th International Conference on Probabilistic
  Graphical Models
volume: '246'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 9
  - 5
pdf: https://raw.githubusercontent.com/mlresearch/v246/main/assets/ghandi24a/ghandi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
